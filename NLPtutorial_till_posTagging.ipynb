{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dc088ad-a282-4d92-bebd-b6eee5ee18e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"Hello I'm TANISHA RAY. I'm learning NLP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37d110f0-9903-42a1-84fa-9ccb1eb3be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97884097-c0ac-4678-997a-44bc8e5fbc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm TANISHA RAY. I'm learning NLP.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9720e56-8cdf-442c-b80a-aa02940f9abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dac17e54-051f-4326-9fcf-8a003caea10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed192f69-bed6-4cd3-9225-ab17f77df2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello I'm TANISHA RAY.\n",
      "I'm learning NLP.\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2c946d02-1521-41c8-9ec1-4ad24a42901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd182002-3c97-4d01-82f2-6b5c38e53cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'TANISHA',\n",
       " 'RAY',\n",
       " '.',\n",
       " 'I',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'learning',\n",
       " 'NLP',\n",
       " '.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6b4c605-c5c9-4555-b755-f289ade83d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', \"'\", 'm', 'TANISHA', 'RAY', '.']\n",
      "['I', \"'\", 'm', 'learning', 'NLP', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(wordpunct_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c211e8f3-792b-4b91-a979-d9cbcd48e84c",
   "metadata": {},
   "source": [
    "#Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "85427d62-ade0-4197-84b5-e963aa289fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= [\"eat\", \"eating\", \"eats\", \"laugh\", \"laughs\", \"laughing\", \"done ^_^ \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e3ef5ed4-5409-40f5-8f18-2a6b96ce6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "14926334-f528-449e-b6a2-f3f4762d03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedword= PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0fa0872-a7fb-4cbc-8e13-1f6ae65f7738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "eat\n",
      "laugh\n",
      "laugh\n",
      "laugh\n",
      "done ^_^ \n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(stemmedword.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755aba5c-7b03-452e-b3ef-ecc6ccd8cad0",
   "metadata": {},
   "source": [
    "snowball stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac7fcd4-f6f9-49af-8d65-3c3f740fe992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbd6f8e4-9e4a-472b-84a9-c5216fc33e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= [\"eating\", \"eats\", \"laugh\", \"laughs\", \"laughing\", \"goes\", \"going\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6683830a-5e2f-44d3-9df6-7077f588efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowstemming= SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "211f1e88-dc33-467e-8583-4fdb2e6f1881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "laugh\n",
      "laugh\n",
      "laugh\n",
      "goe\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(snowstemming.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bc5ed8-16ea-45e3-a506-3b5e3179ef36",
   "metadata": {},
   "source": [
    "sometimes it doesn't work, like here in case of \"goes\". then we go for lemmatizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "992824f3-4ac7-4d79-a2bf-0a8069a1d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62c4f54d-e756-4110-a33c-546bd9ce798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a63b5462-ca2b-4ced-b604-ad7576ab9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3c78d5e9-a460-42c4-8c54-65d192cf8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "858d1436-6fb0-4fa6-a011-391a81c01d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating\n",
      "eats\n",
      "laugh\n",
      "laugh\n",
      "laughing\n",
      "go\n",
      "going\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379081c0-724a-4f35-994d-9bb9dca8c3a7",
   "metadata": {},
   "source": [
    "default taking pos tag as noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08dd4625-1124-4ada-b8eb-4f9c387f6f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eat\n",
      "laugh\n",
      "laugh\n",
      "laugh\n",
      "go\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279a006-56f1-4e6d-a563-b8d8caffc209",
   "metadata": {},
   "source": [
    "pos tag to verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c398691-5aee-4549-b923-dc619fb97e66",
   "metadata": {},
   "source": [
    "#stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "72ac733f-fd99-4bac-838c-4de2f5708e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speech Of DR APJ Abdul Kalam\n",
    "paragraph = \"\"\"In 3000 years of our history, people from all over \n",
    "               the world have come and invaded us, captured our lands, conquered our minds. \n",
    "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
    "               the French, the Dutch, all of them came and looted us, took over what was ours. \n",
    "               Yet we have not done this to any other nation. We have not conquered anyone. \n",
    "               We have not grabbed their land, their culture, \n",
    "               their history and tried to enforce our way of life on them. \n",
    "               Why? Because we respect the freedom of others.That is why my \n",
    "               first vision is that of freedom. I believe that India got its first vision of \n",
    "               this in 1857, when we started the War of Independence. It is this freedom that\n",
    "               we must protect and nurture and build on. If we are not free, no one will respect us.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cab2d465-68d2-4b7b-a086-113081094b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3422cede-6879-4983-be1a-861e7af62a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "62dd7ccd-ed3e-442f-bccf-3aadf6bbbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=stopwords.words('english')\n",
    "#list of all the stopwords that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d21ee98-a150-4ced-a388-b396a138676c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'même',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'à',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'été',\n",
       " 'étée',\n",
       " 'étées',\n",
       " 'étés',\n",
       " 'étant',\n",
       " 'étante',\n",
       " 'étants',\n",
       " 'étantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'êtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'étais',\n",
       " 'était',\n",
       " 'étions',\n",
       " 'étiez',\n",
       " 'étaient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fûmes',\n",
       " 'fûtes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fût',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eûmes',\n",
       " 'eûtes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eût',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0ded8-76f6-425d-be3f-fd4c7cc08576",
   "metadata": {},
   "source": [
    "but it is preferred that we define our own stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be46c531-8461-4b9b-979a-17f6539b7271",
   "metadata": {},
   "source": [
    "##using_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e5186260-e83a-422d-8b91-50a4671be369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5723368c-e428-4ba1-99a5-2181a1384c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowstemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f90bb5ba-fb34-4fd3-a8e9-939651eb1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3cafffa8-14f4-422c-a830-24c957875631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "49081251-b33d-466b-84d8-6d1e2484b668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i]) #first separating sentences out of paragraph\n",
    "    words=[snowstemmer.stem(word) for word in words if word not in set(stopwords)] #using set, as it stores unique number #stemming\n",
    "    sentences[i]=' '.join(words)  #converting all the list of words into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d51b486d-6fbe-4538-88ba-60bfd821c9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
       " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
       " 'yet done nation .',\n",
       " 'we conquer anyon .',\n",
       " 'we grab land , cultur , histori tri enforc way life .',\n",
       " 'whi ?',\n",
       " 'becaus respect freedom others.that first vision freedom .',\n",
       " 'i believ india got first vision 1857 , start war independ .',\n",
       " 'it freedom must protect nurtur build .',\n",
       " 'if free , one respect us .']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9703f2a-c21e-487a-97bb-b0d572093213",
   "metadata": {},
   "source": [
    "#now we are using lemmaatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "66a9cd88-b588-4427-bb73-f48f6e39cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)  \n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i]) #first separating sentences out of paragraph\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)] #using set, as it stores unique number #stemming\n",
    "    sentences[i]=' '.join(words)  #converting all the list of words into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "f071cd40-5e7b-4b75-be51-fc50c0641bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In 3000 year history , people world come invaded u , captured land , conquered mind .',\n",
       " 'From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .',\n",
       " 'Yet done nation .',\n",
       " 'We conquered anyone .',\n",
       " 'We grabbed land , culture , history tried enforce way life .',\n",
       " 'Why ?',\n",
       " 'Because respect freedom others.That first vision freedom .',\n",
       " 'I believe India got first vision 1857 , started War Independence .',\n",
       " 'It freedom must protect nurture build .',\n",
       " 'If free , one respect u .']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "18b3a1fe-3ff8-44d0-8a64-9cf6ec657ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\tanis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "4d355630-588e-492c-826b-c016dff96cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('3000', 'CD'), ('year', 'NN'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('u', 'JJ'), (',', ','), ('captured', 'JJ'), ('land', 'NN'), (',', ','), ('conquered', 'VBD'), ('mind', 'NN'), ('.', '.')]\n",
      "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'VBN'), ('u', 'NN'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
      "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
      "[('Why', 'WRB'), ('?', '.')]\n",
      "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
      "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
      "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('u', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i]) #first separating sentences out of paragraph\n",
    "    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords)]\n",
    "    postag=nltk.pos_tag(words)\n",
    "    print(postag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8a60b7-c8c5-47b4-8cf4-cb4e77b50df4",
   "metadata": {},
   "source": [
    "\n",
    "#some tag examples\n",
    "CC: Coordinating conjunction (e.g. “and”, “or”, “but”)\n",
    "CD: Cardinal number (e.g. “one”, “two”, “three”)\n",
    "DT: Determiner (e.g. “the”, “a”, “an”)\n",
    "EX: Existential there (e.g. “there”)\n",
    "FW: Foreign word (e.g. “bonjour”, “guten tag”)\n",
    "IN: Preposition or subordinating conjunction (e.g. “in”, “on”, “before”)\n",
    "JJ: Adjective (e.g. “big”, “small”, “happy”)\n",
    "JJR: Adjective, comparative (e.g. “bigger”, “smaller”, “happier”)\n",
    "JJS: Adjective, superlative (e.g. “biggest”, “smallest”, “happiest”)\n",
    "LS: List item marker (e.g. “1”, “2”, “3”)\n",
    "NN: Noun, singular or mass (e.g. “dog”, “book”, “desk”)\n",
    "NNS: Noun, plural (e.g. “dogs”, “books”, “desks”)\n",
    "NNP: Proper noun, singular (e.g. “John”, “Paris”, “Monday”)\n",
    "NNPS: Proper noun, plural (e.g. “Johns”, “Parises”, “Mondays”)\n",
    "RB: Adverb (e.g. “quickly”, “slowly”, “happily”)\n",
    "RBR: Adverb, comparative (e.g. “quicker”, “slower”, “happier”)\n",
    "RBS: Adverb, superlative (e.g. “quickest”, “slowest”, “happiest”)\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "abc0e6e8-0d74-4692-bce7-4542a62f15d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She', 'is', 'such', 'an', 'intelligent', 'girl.']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"She is such an intelligent girl.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "ee367e54-34e2-47bc-af13-48ca83f0052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRP'), ('is', 'VBZ'), ('such', 'JJ'), ('an', 'DT'), ('intelligent', 'JJ'), ('girl.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"She is such an intelligent girl.\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079087dc-6b3e-4e0c-9ff4-ed6784b6b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
